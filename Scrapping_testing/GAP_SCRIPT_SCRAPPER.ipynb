{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.join(os.path.dirname('__file__'), '..', 'DB_and_Azure'))\n",
        "import sql_db_functions as SQLf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_price(text):\n",
        "\n",
        "    # Remove any non-numeric characters except for ',' and '.'\n",
        "    cleaned_text = re.sub(r'[^\\d,\\.]', '', text)\n",
        "    \n",
        "    # Replace comma with a period if there's no period already (to handle decimal part)\n",
        "    if ',' in cleaned_text and '.' not in cleaned_text:\n",
        "        cleaned_text = cleaned_text.replace(',', '.')\n",
        "\n",
        "    elif ',' not in cleaned_text and '.' in cleaned_text:\n",
        "        cleaned_text = cleaned_text.replace('.', '')\n",
        "\n",
        "    elif ',' in cleaned_text and '.' in cleaned_text:\n",
        "        # If both ',' and '.' are present, keep only the period as the decimal separator\n",
        "        cleaned_text = cleaned_text.replace('.', '')\n",
        "        cleaned_text = cleaned_text.replace(',', '.')\n",
        "    \n",
        "    # Convert the string to a float\n",
        "    number = float(cleaned_text)\n",
        "    \n",
        "    return number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qzo2PfUGrfIi"
      },
      "outputs": [],
      "source": [
        "def scrape_gap_product(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    product = {}\n",
        "\n",
        "    # Extract product name\n",
        "    title_element = soup.find('h1', class_='pdp-mfe-1x22u9v')\n",
        "    if title_element:\n",
        "        product['name'] = title_element.text.strip()\n",
        "\n",
        "    # Extract product price\n",
        "    price_section = soup.find('div', class_='pdp-pricing pdp-mfe-1jiw3bl')\n",
        "    if price_section:\n",
        "        current_price_element = price_section.find('span', class_='pdp-pricing--highlight pdp-pricing__selected pdp-mfe-1jiw3bl')\n",
        "        original_price_element = price_section.find('span', class_='product-price--pdp__regular')\n",
        "        if current_price_element:\n",
        "            product['current_price'] = current_price_element.text.strip()\n",
        "        if original_price_element:\n",
        "            product['original_price'] = original_price_element.text.strip()\n",
        "\n",
        "    # Extract product color\n",
        "    color_element = soup.find('span', class_='swatch-label__value')\n",
        "    if color_element:\n",
        "        product['color'] = color_element.text.strip()\n",
        "\n",
        "    # Extract product sizes\n",
        "    sizes = []\n",
        "    size_elements = soup.select('.pdp-variant_radio input')\n",
        "    for size in size_elements:\n",
        "        if not size.has_attr('disabled'):\n",
        "            sizes.append(size['value'])\n",
        "    product['sizes'] = sizes\n",
        "\n",
        "    # Extract first three product images\n",
        "    images = []\n",
        "    image_elements = soup.select('img.pdp-images__image')[:3]\n",
        "    for img in image_elements:\n",
        "        img_url = urljoin(url, img['src'])\n",
        "        images.append(img_url)\n",
        "    product['images'] = images\n",
        "\n",
        "    # Extract product description\n",
        "    description_element = soup.find('p', class_='product-information-item__overview')\n",
        "    if description_element:\n",
        "        product['description'] = description_element.text.strip()\n",
        "\n",
        "    # Extract product details\n",
        "    details_element = soup.find('ul', class_='product-information-item__list')\n",
        "    if details_element:\n",
        "        product_details = [li.text.strip() for li in details_element.find_all('li')]\n",
        "        product['details'] = product_details\n",
        "\n",
        "    return product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_gap_catalog(catalog_url):\n",
        "    response = requests.get(catalog_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the catalog page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract product page URLs\n",
        "    product_links = soup.select('a.product-card__link')\n",
        "    product_urls = [urljoin(catalog_url, link['href']) for link in product_links]\n",
        "\n",
        "    all_products = []\n",
        "\n",
        "    for product_url in product_urls:\n",
        "        retry_count = 0\n",
        "        while retry_count < 3:\n",
        "            product_data = scrape_gap_product(product_url)\n",
        "            if product_data:\n",
        "                all_products.append(product_data)\n",
        "                break\n",
        "            else:\n",
        "                retry_count += 1\n",
        "                time.sleep(5)  # wait 5 seconds before retrying\n",
        "\n",
        "        break\n",
        "\n",
        "        # Delay to avoid being blocked\n",
        "        time.sleep(random.uniform(2, 5))  # Random delay between 2 and 5 seconds\n",
        "\n",
        "    return all_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "catalog_url = 'https://www.gap.com/browse/category.do?cid=34608&nav=meganav%3AWomen%3ACategories%3AShirts%20%26%20Tops#pageId=0&department=136'\n",
        "\n",
        "catalog_url = 'https://www.gap-italia.it/it/c/donna/bluse-e-camicie/'\n",
        "catalog_data = scrape_gap_catalog(catalog_url)\n",
        "\n",
        "if catalog_data:\n",
        "    print(catalog_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "catalog_data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
