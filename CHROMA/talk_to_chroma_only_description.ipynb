import sys, os
sys.path.append(os.path.join(os.path.dirname('__file__'), '..', 'DB_and_Azure'))
import sql_db_functions as SQLf
import Chroma_db_functions as Cf

from apikey import apikey

from langchain.vectorstores import Chroma
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,
)

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

from chromadb.config import Settings
import uuid

persist_directory = '../../../Chromadb/chroma-db-full-descriptions'
embedding = Cf.get_embeddings()

db = Chroma(persist_directory= persist_directory, embedding_function=embedding,collection_name='encodings', )

#Ensure we are pulling data correctly
db.similarity_search(query='hello')

retriever = db.as_retriever(search_kwargs={"k": 5})

"""### Gpt"""

os.environ['OPENAI_API_KEY'] = apikey

turbo_llm = ChatOpenAI(
    temperature=0,
    model_name='gpt-3.5-turbo'
)

"""# Test chain
Setting up -> https://python.langchain.com/v0.2/docs/tutorials/rag/
"""

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain import hub

prompt_hub = hub.pull("rlm/rag-prompt")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt_hub
    | turbo_llm
    | StrOutputParser()
)

### lets set up the system

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

system_prompt = (
    "You are a search engine for clothing. "
    "Use the following pieces of retrieved context to suggest the best peace of clothing "
    "the question. If you don't know the answer, say that you "
    "answer concise."
    "Explain why the 5 selected options of the context are the best ones"
    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(turbo_llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

query = "Brand: HM / Type: Dress   / Fit: Regular fit   / Sleeve style: Long sleeve   / Neckline: V-neck   / Material: Cotton   / Formality: Casual   / Seasson: Autumn   / Colors: 100% Black   / Texture: Smooth   / Transparency: Opaque   / Details and Embellishments: Buttons   / Shape: Fitted   / Length: Midi   / Collar Style: Not applicable   / Sleeve Style: Long   / Patterns: Not applicable   / Patterns placement: Not applicable   / Fluidity of fabric: Moderate   / Fabric weight: Medium   / Pocket Presence: No   / Pocket placement: Not applicable   / Pocket size: Not applicable   / Breathability: Medium   / Occasion Suitability: Casual   / Lapel: Not applicable'"

query = "/ Material: Cotton   / Formality: Casual   / Seasson: Autumn   / Colors: 100% Black   / Texture: Smooth   / Transparency: Opaque   / Details and Embellishments: Buttons / Sleeve Style: Long / Patterns placement: Not applicable   / Fluidity of fabric: Moderate   / Fabric weight: Medium   / Pocket Presence: No   / Pocket placement: Not applicable   / Pocket size: Not applicable   / Breathability: Medium   / Occasion Suitability: Casual   / Lapel: Not applicable'"

response = rag_chain.invoke({"input": query})
print(response["answer"])

for document in response["context"]:
    print(document)
    print()

"""# testing 2nd"""

from langchain.storage import InMemoryStore
from langchain.retrievers.multi_vector import MultiVectorRetriever

persist_directory = '../../../Chromadb/chroma-db-full-description'
embedding = Cf.get_embeddings()

vectorstore = Chroma(
    collection_name="multi_modal_rag", embedding_function=embedding,persist_directory = persist_directory
)

#vectorstore.similarity_search('summer')

#store = InMemoryStore()
id_key = 'doc_id'

retriever = vectorstore.as_retriever(search_kwargs={"k":5})

#retriever = MultiVectorRetriever(
#    vectorstore=vectorstore,
#    docstore=store,
#    id_key=id_key,
#    search_kwargs={"k":5}
#)

docs = retriever.invoke(
    "Gucci"
)

#docs

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain import hub

prompt_hub = hub.pull("rlm/rag-prompt")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt_hub
    | turbo_llm
    | StrOutputParser()
)

"""### chain"""

### lets set up the system

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

system_prompt = (
    "You are a search engine for clothing. "
    "Use the following pieces of retrieved context to suggest the best peace of clothing "
    "the question. If you don't know the answer, say that you "
    "Explain each of the 5 selected options of the context are the best ones in a consice matter"
    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(turbo_llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

generalized_description = 'This silk shirt features an optical monogram print with a regular fit and long sleeves ending in buttoned cuffs. It has a classic pointed collar that adds a touch of formality. The high-quality silk fabric provides a luxurious feel and fluid drape, making it suitable for all seasons due to its breathable and lightweight nature. The deep lavender and white colors are intricately woven to create a striking visual effect, with consistent pattern placement for a cohesive look. The smooth, slightly glossy texture typical of silk, together with its non-transparent quality, makes the shirt appropriate for both formal and semi-formal occasions. Absent of pockets, embellishments, or lapels, the design maintains a sleek and streamlined silhouette. The shirt has a standard length that typically falls just below the waistline and is finished with concealed buttons for a minimalist aesthetic'

query = f"find piece that could be similar to this description: {generalized_description}"

response = rag_chain.invoke({"input": query})
print(response["answer"])

response

i = 2
print(response["context"][i].page_content)
print(response["context"][i].metadata)

def search_similarity_from_description(retriever, description_generalization ):


        #Setting up retriever

        #Getting a generalization of the description

        os.environ['OPENAI_API_KEY'] = apikey

        turbo_llm = ChatOpenAI(
            temperature=0,
            model_name='gpt-3.5-turbo'
        )

        system_prompt = (
            "You are a search engine for clothing. "
            "Use the following pieces of retrieved context to suggest the best peace of clothing "
            "the question. If you don't know the answer, say that you "
            "Explain each of the 5 selected options of the context are the best ones in a consice matter"
            "\n\n"
            "{context}"

        )

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "{input}"),
            ]
        )

        question_answer_chain = create_stuff_documents_chain(turbo_llm, prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        query = f"Find piece that could be similar to this description: {description_generalization}"

        response = rag_chain.invoke({"input": query})

        return response

search_similarity_from_description(retriever=retriever,description_generalization=generalized_description)
